{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport warnings\nimport joblib\n\nwarnings.filterwarnings(\"ignore\")\n# %matplotlib inline # Not needed for script execution\n\n# --- 1. Setup and Configuration ---\nEPW_FILE = \"/home/ubuntu/upload/Torino_IT-hour.epw\"\nSIM_FILE = \"/home/ubuntu/upload/eplusout.csv\"\nos.makedirs(\"output_lstm\", exist_ok=True)\nN_LAG = 24\nSPLIT_RATIO = 0.8\nEPOCHS = 50\nBATCH_SIZE = 64\nQUANTILES = [0.1, 0.5, 0.9]\n\n# --- 2. Data Loading and Preprocessing ---\nprint(\"Loading and preprocessing data...\")\ntry:\n    epw = pd.read_csv(EPW_FILE, skiprows=8, header=None)\n    epw.columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"DataSource\", \"DryBulb\", \"DewPoint\", \"RH\", \"AtmosPressure\", \n                   \"ExtGlobHorRad\", \"ExtDirNormRad\", \"ExtDifHorRad\", \"GlobalHorRad\", \"DirectNormRad\", \n                   \"DiffuseHorRad\", \"InfraSky\", \"WindDir\", \"WindSpd\", \"TotalSkyCover\", \"OpaqueSkyCover\", \n                   \"Visibility\", \"CeilingHeight\", \"PresWeatherObs\", \"PresWeatherCodes\", \"PrecipWater\", \"AerosolOptDepth\",\n                   \"SnowDepth\", \"DaysSinceSnow\", \"Albedo\", \"LiquidPrecip\", \"RainRate\", \"RainDuration\", \"SnowRate\", \"SnowDuration\"]\n    weather_df = epw[[\"DryBulb\", \"RH\", \"ExtDirNormRad\", \"ExtDifHorRad\"]].copy()\n    weather_df.columns = [\"Temperature\", \"Humidity\", \"DirectRad\", \"DiffuseRad\"]\nexcept FileNotFoundError:\n    print(f\"Error: EPW file not found at {EPW_FILE}\")\n    exit()\n\ntry:\n    sim_df = pd.read_csv(SIM_FILE, low_memory=False)\n    target_col = \"BLOCCO1:ZONA3:Zone Total Internal Latent Gain Energy [J](TimeStep)\"\n    if target_col not in sim_df.columns:\n        print(f\"Error: Target column \\t{target_col}\\t not found in {SIM_FILE}\")\n        energy_cols = [c for c in sim_df.columns if \"Energy\" in c and \"Latent\" in c]\n        if energy_cols:\n            target_col = energy_cols[0]\n            print(f\"Using fallback target column: {target_col}\")\n        else:\n            print(\"No suitable energy column found. Exiting.\")\n            exit()\n    sim_df_hourly = sim_df[[target_col]].copy()\nexcept FileNotFoundError:\n    print(f\"Error: Simulation file not found at {SIM_FILE}\")\n    exit()\n\nweather_df_reset = weather_df.reset_index(drop=True)\nsim_df_hourly_reset = sim_df_hourly.reset_index(drop=True)\nmin_length = min(len(weather_df_reset), len(sim_df_hourly_reset))\nweather_df_reset = weather_df_reset.iloc[:min_length]\nsim_df_hourly_reset = sim_df_hourly_reset.iloc[:min_length]\nfull_df = pd.concat([weather_df_reset, sim_df_hourly_reset], axis=1).dropna()\nprint(f\"Combined dataset shape: {full_df.shape}\")\n\n# --- 3. Feature Scaling and Sequence Creation ---\nfeatures = full_df[[\"Temperature\", \"Humidity\", \"DirectRad\", \"DiffuseRad\"]].values\ntarget = full_df[target_col].values\nscaler_X = MinMaxScaler()\nscaler_Y = MinMaxScaler()\nX_scaled = scaler_X.fit_transform(features)\nY_scaled = scaler_Y.fit_transform(target.reshape(-1, 1))\nX_seq, Y_seq = [], []\nfor i in range(N_LAG, len(X_scaled)):\n    X_seq.append(X_scaled[i - N_LAG:i])\n    Y_seq.append(Y_scaled[i])\nX_seq = np.array(X_seq)\nY_seq = np.array(Y_seq)\nsplit_idx = int(SPLIT_RATIO * len(X_seq))\nX_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\nY_train, Y_test = Y_seq[:split_idx], Y_seq[split_idx:]\nprint(f\"Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\nprint(f\"Testing data shape: X={X_test.shape}, Y={Y_test.shape}\")\n\n# --- 4. LSTM Model with Quantile Loss ---\ndef quantile_loss(q, y_true, y_pred):\n    e = y_true - y_pred\n    return keras.backend.mean(keras.backend.maximum(q * e, (q - 1) * e), axis=-1)\n\ndef build_lstm_quantile_model(n_timesteps, n_features, n_quantiles):\n    model = keras.Sequential()\n    model.add(layers.LSTM(50, activation=\"relu\", input_shape=(n_timesteps, n_features), return_sequences=True))\n    model.add(layers.LSTM(50, activation=\"relu\"))\n    model.add(layers.Dense(50, activation=\"relu\"))\n    model.add(layers.Dense(n_quantiles))\n    return model\n\nn_features = X_train.shape[2]\nn_quantiles = len(QUANTILES)\nlstm_model = build_lstm_quantile_model(N_LAG, n_features, n_quantiles)\nlosses = [lambda y_true, y_pred, q=q: quantile_loss(q, y_true, y_pred) for q in QUANTILES]\nlstm_model.compile(optimizer=\"adam\", loss=losses)\nlstm_model.summary()\n\n# --- 5. Train the LSTM Model ---\nprint(\"Training LSTM model...\")\nhistory = lstm_model.fit(\n    X_train,\n    Y_train, \n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_split=0.2,\n    verbose=1,\n    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)]\n)\nprint(\"Training complete.\")\nplt.figure(figsize=(10, 6))\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.title(\"LSTM Model Training History\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"output_lstm/training_history.png\")\n# plt.show() # Avoid showing plot\n\n# --- 6. Evaluate the Model and Visualize Predictions ---\nprint(\"Evaluating model on test data...\")\nY_pred_quantiles_scaled = lstm_model.predict(X_test)\nY_pred_quantiles = scaler_Y.inverse_transform(Y_pred_quantiles_scaled)\nY_test_actual = scaler_Y.inverse_transform(Y_test)\nY_pred_lower = Y_pred_quantiles[:, 0]\nY_pred_median = Y_pred_quantiles[:, 1]\nY_pred_upper = Y_pred_quantiles[:, 2]\nmae = mean_absolute_error(Y_test_actual, Y_pred_median)\nrmse = np.sqrt(mean_squared_error(Y_test_actual, Y_pred_median))\nprint(f\"Test MAE (Median Prediction): {mae:.2f}\")\nprint(f\"Test RMSE (Median Prediction): {rmse:.2f}\")\n\nplt.figure(figsize=(15, 7))\nplt.plot(Y_test_actual, label=\"True Energy\", color=\"black\")\nplt.plot(Y_pred_median, label=\"Predicted Median (q=0.5)\", color=\"purple\")\nplt.fill_between(range(len(Y_test_actual)), Y_pred_lower, Y_pred_upper, color=\"purple\", alpha=0.2, label=\"80% Prediction Interval (q=0.1 to 0.9)\")\nplt.title(\"LSTM Energy Prediction with Uncertainty (Quantile Regression)\")\nplt.xlabel(\"Time Step (Hour)\")\nplt.ylabel(\"Energy (J)\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"output_lstm/prediction_vs_actual_quantile.png\")\n# plt.show() # Avoid showing plot\n\nplt.figure(figsize=(15, 7))\nplt.plot(Y_test_actual[:100], label=\"True Energy\", color=\"black\")\nplt.plot(Y_pred_median[:100], label=\"Predicted Median (q=0.5)\", color=\"purple\")\nplt.fill_between(range(100), Y_pred_lower[:100], Y_pred_upper[:100], color=\"purple\", alpha=0.2, label=\"80% Prediction Interval (q=0.1 to 0.9)\")\nplt.title(\"LSTM Energy Prediction - First 100 Hours\")\nplt.xlabel(\"Time Step (Hour)\")\nplt.ylabel(\"Energy (J)\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"output_lstm/prediction_vs_actual_quantile_100h.png\")\n# plt.show() # Avoid showing plot\n\n# --- 7. Save Model and Scalers ---\nlstm_model.save(\"/home/ubuntu/lstm_energy_predictor.keras\")\njoblib.dump(scaler_X, \"/home/ubuntu/scaler_X.joblib\")\njoblib.dump(scaler_Y, \"/home/ubuntu/scaler_Y.joblib\")\nprint(\"Model and scalers saved.\")\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}