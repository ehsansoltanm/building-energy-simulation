{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Weather and Energy Prediction with Uncertainty Quantification\n",
    "\n",
    "- Implements simplified prediction model with MFU and MFI approaches\n",
    "- Uses scikit-learn for modeling instead of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to data files (Ensure these files are in the same directory as the notebook)\n",
    "EPW_FILE = \"Torino_IT-hour.epw\"\n",
    "SIM_FILE = \"eplusout.csv\"\n",
    "\n",
    "# Create output directory for plots\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "\n",
    "# Load EPW weather data\n",
    "epw = pd.read_csv(EPW_FILE, skiprows=8, header=None)\n",
    "epw.columns = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\", \"DataSource\", \"DryBulb\", \"DewPoint\", \"RH\", \"AtmosPressure\", \n",
    "               \"ExtGlobHorRad\", \"ExtDirNormRad\", \"ExtDifHorRad\", \"GlobalHorRad\", \"DirectNormRad\", \n",
    "               \"DiffuseHorRad\", \"InfraSky\", \"WindDir\", \"WindSpd\", \"TotalSkyCover\", \"OpaqueSkyCover\", \n",
    "               \"Visibility\", \"CeilingHeight\", \"PresWeatherObs\", \"PresWeatherCodes\", \"PrecipWater\", \"AerosolOptDepth\",\n",
    "               \"SnowDepth\", \"DaysSinceSnow\", \"Albedo\", \"LiquidPrecip\", \"RainRate\", \"RainDuration\", \"SnowRate\", \"SnowDuration\"]\n",
    "\n",
    "# Select relevant weather features\n",
    "weather_df = epw[[\"DryBulb\", \"RH\", \"ExtDirNormRad\", \"ExtDifHorRad\"]].copy()\n",
    "weather_df.columns = [\"Temperature\", \"Humidity\", \"DirectRad\", \"DiffuseRad\"]\n",
    "\n",
    "# Load simulation output data\n",
    "sim_df = pd.read_csv(SIM_FILE, low_memory=False)\n",
    "\n",
    "# Print available columns to help with debugging\n",
    "print(\"Available columns in simulation data:\")\n",
    "energy_columns = [col for col in sim_df.columns if 'Energy' in col]\n",
    "temperature_columns = [col for col in sim_df.columns if 'Temperature' in col]\n",
    "print(f\"Found {len(energy_columns)} energy-related columns\")\n",
    "print(f\"Found {len(temperature_columns)} temperature-related columns\")\n",
    "\n",
    "# Select a suitable target column (Zone Total Internal Latent Gain Energy for BLOCCO1:ZONA3)\n",
    "target_col = \"BLOCCO1:ZONA3:Zone Total Internal Latent Gain Energy [J](TimeStep)\"\n",
    "print(f\"Selected target variable: {target_col}\")\n",
    "\n",
    "# Select hourly data from simulation output\n",
    "sim_df_hourly = sim_df[[target_col]].copy()\n",
    "\n",
    "# Combine weather and simulation data\n",
    "# Reset index to ensure proper alignment\n",
    "weather_df_reset = weather_df.reset_index(drop=True)\n",
    "sim_df_hourly_reset = sim_df_hourly.reset_index(drop=True)\n",
    "\n",
    "# Ensure both dataframes have the same length\n",
    "min_length = min(len(weather_df_reset), len(sim_df_hourly_reset))\n",
    "weather_df_reset = weather_df_reset.iloc[:min_length]\n",
    "sim_df_hourly_reset = sim_df_hourly_reset.iloc[:min_length]\n",
    "\n",
    "full_df = pd.concat([weather_df_reset, sim_df_hourly_reset], axis=1).dropna()\n",
    "\n",
    "print(f\"Combined dataset shape: {full_df.shape}\")\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lag for time series prediction\n",
    "n_lag = 24  # 24 hours (1 day)\n",
    "\n",
    "# Extract features and target\n",
    "features = full_df[[\"Temperature\", \"Humidity\", \"DirectRad\", \"DiffuseRad\"]].values\n",
    "target = full_df[target_col].values\n",
    "\n",
    "# Normalize data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(features)\n",
    "Y_scaled = scaler_Y.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "# Create time sequences for input\n",
    "X_seq, Y_seq = [], []\n",
    "for i in range(n_lag, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i - n_lag:i])\n",
    "    Y_seq.append(Y_scaled[i])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_seq = np.array(X_seq)\n",
    "Y_seq = np.array(Y_seq)\n",
    "\n",
    "# Reshape for sklearn models (flatten the time steps and features)\n",
    "X_seq_flat = X_seq.reshape(X_seq.shape[0], -1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "split = int(0.8 * len(X_seq_flat))\n",
    "X_train, X_test = X_seq_flat[:split], X_seq_flat[split:]\n",
    "Y_train, Y_test = Y_seq[:split], Y_seq[split:]\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {Y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Basic Prediction Model (Replacement for LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining basic prediction model...\")\n",
    "\n",
    "# Use MLPRegressor as a simplified alternative to LSTM\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, Y_train.ravel())\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "# Convert back to original scale\n",
    "Y_test_inv = scaler_Y.inverse_transform(Y_test)\n",
    "predictions_inv = scaler_Y.inverse_transform(predictions)\n",
    "\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(Y_test_inv, predictions_inv)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test_inv, predictions_inv))\n",
    "r2 = r2_score(Y_test_inv, predictions_inv)\n",
    "\n",
    "print(f\"Model Evaluation:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"RÂ²: {r2:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Y_test_inv[:100], label=\"True\", color='black')\n",
    "plt.plot(predictions_inv[:100], label=\"Predicted\", color='blue')\n",
    "plt.legend()\n",
    "plt.title(\"Basic Prediction Model (First 100 hours)\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.grid()\n",
    "plt.savefig(\"output/basic_prediction.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Model-Free Uncertainty (MFU) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nImplementing Model-Free Uncertainty (MFU) with ensemble methods...\")\n",
    "\n",
    "# Create ensemble of models\n",
    "n_models = 10\n",
    "ensemble_models = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    # Create model with different random initialization\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=i\n",
    "    )\n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    ensemble_models.append(model)\n",
    "    print(f\"  Trained model {i+1}/{n_models}\")\n",
    "\n",
    "# Make predictions with all models\n",
    "ensemble_predictions = np.array([model.predict(X_test).reshape(-1, 1) for model in ensemble_models])\n",
    "\n",
    "# Calculate mean prediction and uncertainty (standard deviation)\n",
    "mean_prediction = np.mean(ensemble_predictions, axis=0)\n",
    "uncertainty = np.std(ensemble_predictions, axis=0)\n",
    "\n",
    "# Convert back to original scale\n",
    "mean_prediction_inv = scaler_Y.inverse_transform(mean_prediction)\n",
    "uncertainty_inv = uncertainty * (scaler_Y.data_max_ - scaler_Y.data_min_)\n",
    "\n",
    "# Visualize predictions with uncertainty\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Y_test_inv[:100], label=\"True\", color='black')\n",
    "plt.plot(mean_prediction_inv[:100], label=\"Predicted\", color='blue')\n",
    "plt.fill_between(\n",
    "    range(100),\n",
    "    mean_prediction_inv[:100].flatten() - 2 * uncertainty_inv[:100].flatten(),\n",
    "    mean_prediction_inv[:100].flatten() + 2 * uncertainty_inv[:100].flatten(),\n",
    "    alpha=0.2, color='blue', label=\"95% Confidence Interval\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Prediction with Uncertainty (MFU Ensemble Method)\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.grid()\n",
    "plt.savefig(\"output/mfu_ensemble.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Model-Free Inference (MFI) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nImplementing Model-Free Inference (MFI) with conformal prediction...\")\n",
    "\n",
    "# Split training data into proper training and calibration sets\n",
    "X_proper_train, X_calib, Y_proper_train, Y_calib = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model on proper training set\n",
    "conformal_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "conformal_model.fit(X_proper_train, Y_proper_train.ravel())\n",
    "\n",
    "# Calculate calibration residuals\n",
    "calib_preds = conformal_model.predict(X_calib).reshape(-1, 1)\n",
    "calib_residuals = np.abs(Y_calib - calib_preds)\n",
    "\n",
    "# Function to compute conformal prediction intervals\n",
    "def conformal_prediction_interval(model, X, residuals, alpha=0.05):\n",
    "    # Predict point estimates\n",
    "    y_pred = model.predict(X).reshape(-1, 1)\n",
    "    \n",
    "    # Calculate quantile of residuals\n",
    "    q = np.quantile(residuals, 1 - alpha)\n",
    "    \n",
    "    # Construct prediction intervals\n",
    "    lower_bound = y_pred - q\n",
    "    upper_bound = y_pred + q\n",
    "    \n",
    "    return y_pred, lower_bound, upper_bound\n",
    "\n",
    "# Apply conformal prediction to test data\n",
    "conf_pred, conf_lower, conf_upper = conformal_prediction_interval(\n",
    "    conformal_model, X_test, calib_residuals\n",
    ")\n",
    "\n",
    "# Convert to original scale\n",
    "conf_pred_inv = scaler_Y.inverse_transform(conf_pred)\n",
    "conf_lower_inv = scaler_Y.inverse_transform(conf_lower)\n",
    "conf_upper_inv = scaler_Y.inverse_transform(conf_upper)\n",
    "\n",
    "# Visualize conformal prediction intervals\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Y_test_inv[:100], label=\"True\", color='black')\n",
    "plt.plot(conf_pred_inv[:100], label=\"Predicted\", color='green')\n",
    "plt.fill_between(\n",
    "    range(100),\n",
    "    conf_lower_inv[:100].flatten(),\n",
    "    conf_upper_inv[:100].flatten(),\n",
    "    alpha=0.2, color='green', label=\"95% Prediction Interval\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Prediction with Uncertainty (MFI Conformal Prediction)\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.grid()\n",
    "plt.savefig(\"output/mfi_conformal.png\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate coverage (percentage of true values within prediction intervals)\n",
    "coverage = np.mean((Y_test_inv >= conf_lower_inv) & (Y_test_inv <= conf_upper_inv))\n",
    "print(f\"Conformal prediction empirical coverage: {coverage:.4f} (target: 0.95)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Quantile Regression for Asymmetric Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nImplementing Quantile Regression for asymmetric uncertainty...\")\n",
    "\n",
    "# Train Random Forest models for different quantiles\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "quantile_models = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    # Use Random Forest with quantile loss approximation\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        min_samples_leaf=int(len(X_train) * 0.01),  # Adjust leaf size based on quantile\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, Y_train.ravel())\n",
    "    quantile_models[q] = model\n",
    "\n",
    "# Predict quantiles\n",
    "lower_quantile = quantile_models[0.1].predict(X_test).reshape(-1, 1) - 0.2  # Adjust for 10th percentile\n",
    "median = quantile_models[0.5].predict(X_test).reshape(-1, 1)\n",
    "upper_quantile = quantile_models[0.9].predict(X_test).reshape(-1, 1) + 0.2  # Adjust for 90th percentile\n",
    "\n",
    "# Convert to original scale\n",
    "lower_inv = scaler_Y.inverse_transform(lower_quantile)\n",
    "median_inv = scaler_Y.inverse_transform(median)\n",
    "upper_inv = scaler_Y.inverse_transform(upper_quantile)\n",
    "\n",
    "# Visualize quantile regression predictions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(Y_test_inv[:100], label=\"True\", color='black')\n",
    "plt.plot(median_inv[:100], label=\"Median (q=0.5)\", color='purple')\n",
    "plt.fill_between(\n",
    "    range(100),\n",
    "    lower_inv[:100].flatten(),\n",
    "    upper_inv[:100].flatten(),\n",
    "    alpha=0.2, color='purple', label=\"80% Prediction Interval\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Prediction with Uncertainty (Quantile Regression)\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.grid()\n",
    "plt.savefig(\"output/quantile_regression.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: Out-of-Distribution Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nImplementing Out-of-Distribution Detection...\")\n",
    "\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def detect_ood_samples(X_train, X_test, threshold_percentile=95):\n",
    "    \"\"\"\n",
    "    Detect out-of-distribution samples using Mahalanobis distance\n",
    "    \"\"\"\n",
    "    # Calculate mean and covariance of training data\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    cov = np.cov(X_train, rowvar=False)\n",
    "    \n",
    "    # Add small regularization to ensure covariance matrix is invertible\n",
    "    cov = cov + 1e-6 * np.eye(cov.shape[0])\n",
    "    \n",
    "    # Calculate Mahalanobis distance for each test sample\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: Covariance matrix is singular. Using pseudo-inverse.\")\n",
    "        inv_cov = np.linalg.pinv(cov) # Use pseudo-inverse if matrix is singular\n",
    "        \n",
    "    mahalanobis_distances = np.array([\n",
    "        mahalanobis(x, mean, inv_cov) for x in X_test\n",
    "    ])\n",
    "    \n",
    "    # Set threshold based on percentile of distances from the *test* set (alternative: use distances from a validation set)\n",
    "    threshold = np.percentile(mahalanobis_distances, threshold_percentile)\n",
    "    \n",
    "    # Identify OOD samples\n",
    "    is_ood = mahalanobis_distances > threshold\n",
    "    \n",
    "    return is_ood, mahalanobis_distances, threshold\n",
    "\n",
    "# Apply OOD detection\n",
    "is_ood, distances, threshold = detect_ood_samples(X_train, X_test)\n",
    "\n",
    "# Visualize OOD detection results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(distances[:100], label=\"Mahalanobis Distance\")\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Threshold ({threshold_percentile}th percentile)\")\n",
    "# Highlight OOD samples in the plot\n",
    "ood_indices = np.where(is_ood[:100])[0]\n",
    "plt.scatter(ood_indices, distances[ood_indices], color='red', label='OOD Samples', zorder=5)\n",
    "#plt.fill_between(range(100), 0, distances[:100], where=is_ood[:100], color='red', alpha=0.3, label=\"OOD Samples\") # Alternative visualization\n",
    "plt.legend()\n",
    "plt.title(\"Out-of-Distribution Detection (First 100 hours)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Mahalanobis Distance\")\n",
    "plt.grid()\n",
    "plt.savefig(\"output/ood_detection.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Detected {np.sum(is_ood)} out of {len(is_ood)} samples as out-of-distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Comparison of All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComparing all uncertainty estimation methods...\")\n",
    "\n",
    "# Prepare results from all methods\n",
    "uncertainty_results = {\n",
    "    'Ensemble': {\n",
    "        'mean': mean_prediction_inv.flatten(),\n",
    "        'lower': (mean_prediction_inv - 2 * uncertainty_inv).flatten(),\n",
    "        'upper': (mean_prediction_inv + 2 * uncertainty_inv).flatten()\n",
    "    },\n",
    "    'Conformal': {\n",
    "        'mean': conf_pred_inv.flatten(),\n",
    "        'lower': conf_lower_inv.flatten(),\n",
    "        'upper': conf_upper_inv.flatten()\n",
    "    },\n",
    "    'Quantile': {\n",
    "        'mean': median_inv.flatten(),\n",
    "        'lower': lower_inv.flatten(),\n",
    "        'upper': upper_inv.flatten()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluate all methods\n",
    "def evaluate_uncertainty_methods(y_true, predictions_dict):\n",
    "    \"\"\"\n",
    "    Evaluate different uncertainty estimation methods\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method_name, preds in predictions_dict.items():\n",
    "        # Calculate coverage (percentage of true values within prediction intervals)\n",
    "        coverage = np.mean((y_true >= preds['lower']) & (y_true <= preds['upper']))\n",
    "        \n",
    "        # Calculate interval width (average width of prediction intervals)\n",
    "        interval_width = np.mean(preds['upper'] - preds['lower'])\n",
    "        \n",
    "        # Calculate RMSE of point predictions\n",
    "        rmse = np.sqrt(np.mean((y_true - preds['mean'])**2))\n",
    "        \n",
    "        # Calculate MAPE\n",
    "        # Avoid division by zero or near-zero for MAPE calculation\n",
    "        mask = y_true > 1e-6 # Use a small threshold to identify near-zero true values\n",
    "        if np.sum(mask) > 0:\n",
    "             mape = np.mean(np.abs((y_true[mask] - preds['mean'][mask]) / y_true[mask])) * 100\n",
    "        else:\n",
    "             mape = np.inf # Assign infinity if all true values are near zero\n",
    "        \n",
    "        results[method_name] = {\n",
    "            'coverage': coverage,\n",
    "            'interval_width': interval_width,\n",
    "            'rmse': rmse,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all methods\n",
    "evaluation_results = evaluate_uncertainty_methods(Y_test_inv.flatten(), uncertainty_results)\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame(evaluation_results).T\n",
    "print(\"\\nUncertainty Estimation Methods Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"output/uncertainty_methods_comparison.csv\")\n",
    "\n",
    "# Visualize all methods together (first 100 hours)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot ground truth\n",
    "plt.plot(Y_test_inv[:100], label=\"True\", color='black', linewidth=2)\n",
    "\n",
    "# Plot each method with a different color\n",
    "colors = ['blue', 'green', 'purple']\n",
    "for i, (method_name, preds) in enumerate(uncertainty_results.items()):\n",
    "    plt.plot(preds['mean'][:100], label=f\"{method_name} Mean\", color=colors[i], alpha=0.7)\n",
    "    plt.fill_between(\n",
    "        range(100),\n",
    "        preds['lower'][:100],\n",
    "        preds['upper'][:100],\n",
    "        alpha=0.1, color=colors[i], label=f\"{method_name} Interval\"\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Uncertainty Estimation Methods (First 100 hours)\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.grid()\n",
    "plt.savefig(\"output/comparison_all_methods.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

